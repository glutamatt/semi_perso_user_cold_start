{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glutamatt/semi_perso_user_cold_start/blob/colab/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GsEYdh9xZa9",
        "outputId": "12c861d0-1a6f-429b-ff1b-4d67ba7ab819"
      },
      "source": [
        "!rm -fr repository deezer data\n",
        "!git clone https://github.com/glutamatt/semi_perso_user_cold_start.git repository\n",
        "!cd repository && git checkout colab && cd -"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'repository'...\n",
            "remote: Enumerating objects: 113, done.\u001b[K\n",
            "remote: Counting objects: 100% (113/113), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 113 (delta 60), reused 110 (delta 57), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (113/113), 585.07 KiB | 11.25 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n",
            "Branch 'colab' set up to track remote branch 'colab' from 'origin'.\n",
            "Switched to a new branch 'colab'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvp6bBk8u_yT"
      },
      "source": [
        "# Test de contenu\n",
        "\n",
        "impec \n",
        "\n",
        "## Installation\n",
        "\n",
        "### Code\n",
        "\n",
        "```Bash\n",
        "git clone https://github.com/deezer/carousel_bandits\n",
        "cd carousel_bandits\n",
        "```\n",
        "\n",
        "Requirements: python 3, matplotlib, numpy, pandas, scipy, seaborn\n",
        "\n",
        "Media services providers, such as the music streaming platform [Deezer](https://www.deezer.com/), often leverage **swipeable carousels** to recommend personalized content to their users. These carousels are ranked lists of _L_ items or **cards** from a substantially larger catalog (of size _K_), e.g. _L_ albums, artists or playlists recommended on the homepage of the Deezer app. Only a few cards, say _L_init_ < _L_, are initially displayed to users, who can **swipe** the screen to see additional cards.\n",
        "\n",
        "Selecting the most relevant content to display in carousels is a challenging task, as the catalog is large and as users have different preferences. Also, ranking matters: some cards might not be seen by some users due to the swipeable structure.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img height=\"350\" src=\"https://raw.githubusercontent.com/deezer/carousel_bandits/master/images/carousel.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VLFN2bjpnBY",
        "outputId": "611d0b68-168c-43ea-a938-345470176b49"
      },
      "source": [
        "!pip install fastparquet"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.7/dist-packages (0.6.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2021.6.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.1.5)\n",
            "Requirement already satisfied: cramjam>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.19.5)\n",
            "Requirement already satisfied: thrift>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (0.13.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.1)\n",
            "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.7/dist-packages (from thrift>=0.11.0->fastparquet) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdEFHqZnseg"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn\n",
        "import time\n",
        "import statistics\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.metrics import ndcg_score, dcg_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S1sWV7oy3B"
      },
      "source": [
        "def generate(master_path):\n",
        "    dataset_path = os.getcwd() + \"/repository/data\"\n",
        "\n",
        "    #songs\n",
        "\n",
        "    song_embeddings_path = dataset_path + \"/song_embeddings.parquet\"\n",
        "    song_embeddings = pd.read_parquet(song_embeddings_path, engine = 'fastparquet').fillna(0)\n",
        "\n",
        "    if not os.path.exists(master_path+\"/m_song_dict.pkl\"):\n",
        "        song_dict = {}\n",
        "        for idx, row in song_embeddings.iterrows():\n",
        "            song_dict[row['song_index']] = idx\n",
        "        pickle.dump(song_dict, open(\"{}/m_song_dict.pkl\".format(master_path), \"wb\"))\n",
        "    else:\n",
        "        song_dict = pickle.load(open(\"{}/m_song_dict.pkl\".format(master_path), \"rb\"))\n",
        "\n",
        "\n",
        "    # user embeddings (target = only for train users)\n",
        "\n",
        "    user_embeddings = pd.read_parquet(dataset_path + \"/user_embeddings.parquet\", engine = 'fastparquet')\n",
        "    list_embeddings = [\"embedding_\"+str(i) for i in range(len(user_embeddings[\"svd_embeddings\"][0]))]\n",
        "    user_embeddings[list_embeddings] = pd.DataFrame(user_embeddings.svd_embeddings.tolist(), index= user_embeddings.index)\n",
        "    embeddings_train = user_embeddings[list_embeddings].values\n",
        "\n",
        "    # user features train\n",
        "\n",
        "    features_train_path = dataset_path + \"/user_features_train.parquet\"\n",
        "    features_train = pd.read_parquet(features_train_path, engine = 'fastparquet').fillna(0)\n",
        "    features_train = features_train.sort_values(\"user_index\")\n",
        "    features_train = features_train.reset_index(drop=True)#to check it is ok for train data\n",
        "\n",
        "    # training dataset creation\n",
        "\n",
        "    state = \"train\"\n",
        "    if not os.path.exists(master_path+\"/\"):\n",
        "        os.mkdir(master_path+\"/\")\n",
        "    if not os.path.exists(master_path+\"/\"+state+\"/\"):\n",
        "        os.mkdir(master_path+\"/\"+state+\"/\")\n",
        "    for idx in range(len(features_train)):\n",
        "        x_train = torch.FloatTensor(features_train.iloc[idx,2:])\n",
        "        y_train = torch.FloatTensor(user_embeddings[list_embeddings].iloc[idx,:])\n",
        "        pickle.dump(x_train, open(\"{}/{}/x_train_{}.pkl\".format(master_path, state, idx), \"wb\"))\n",
        "        pickle.dump(y_train, open(\"{}/{}/y_train_{}.pkl\".format(master_path, state, idx), \"wb\"))\n",
        "\n",
        "    # user features validation & test\n",
        "\n",
        "    states = [\"validation\", \"test\"]\n",
        "    for state in states :\n",
        "        features_validation_path = dataset_path + \"/user_features_\" + state + \".parquet\"\n",
        "        features_validation = pd.read_parquet(features_validation_path, engine = 'fastparquet').fillna(0)\n",
        "        features_validation = features_validation.sort_values(\"user_index\")\n",
        "        features_validation = features_validation.reset_index(drop=True)\n",
        "\n",
        "        if not os.path.exists(master_path+\"/\"+state+\"/\"):\n",
        "            os.mkdir(master_path+\"/\"+state+\"/\"+\"/\")\n",
        "        for i in range(len(features_validation)):\n",
        "            x_validation = torch.FloatTensor(features_validation.iloc[i,2:])\n",
        "            y_validation = [song_dict[song_index]  for song_index in features_validation[\"d1d30_songs\"][i]]\n",
        "            groundtruth_validation_list = [1.0 * (song in y_validation) for song in range(len(song_embeddings))]\n",
        "            pickle.dump(x_validation, open(\"{}/{}/x_validation_{}.pkl\".format(master_path, state, i), \"wb\"))\n",
        "            pickle.dump(y_validation, open(\"{}/{}/y_listened_songs_validation_{}.pkl\".format(master_path, state, i), \"wb\"))\n",
        "            pickle.dump(groundtruth_validation_list, open(\"{}/{}/groundtruth_list_{}.pkl\".format(master_path, state, i), \"wb\"))\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp3NF6rMo4SR",
        "outputId": "f44878eb-e53c-4488-d299-3933cdf6e54e"
      },
      "source": [
        "master_path= \"./deezer\"\n",
        "if not os.path.exists(\"{}/\".format(master_path)):\n",
        "  os.mkdir(\"{}/\".format(master_path))\n",
        "  # preparing dataset. It needs about XXGB of your hard disk space.\n",
        "  generate(master_path)\n",
        "else:\n",
        "  print(\"%s already exists\" % master_path)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./deezer already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnfW4DBzmIx",
        "outputId": "62450e2f-43d3-4d64-96e5-c9027798f820",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!apt install -qqy tree && tree deezer"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tree is already the newest version (1.7.0-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "deezer\n",
            "├── m_song_dict.pkl\n",
            "├── test\n",
            "│   ├── groundtruth_list_0.pkl\n",
            "│   ├── groundtruth_list_1.pkl\n",
            "│   ├── groundtruth_list_2.pkl\n",
            "│   ├── groundtruth_list_3.pkl\n",
            "│   ├── groundtruth_list_4.pkl\n",
            "│   ├── x_validation_0.pkl\n",
            "│   ├── x_validation_1.pkl\n",
            "│   ├── x_validation_2.pkl\n",
            "│   ├── x_validation_3.pkl\n",
            "│   ├── x_validation_4.pkl\n",
            "│   ├── y_listened_songs_validation_0.pkl\n",
            "│   ├── y_listened_songs_validation_1.pkl\n",
            "│   ├── y_listened_songs_validation_2.pkl\n",
            "│   ├── y_listened_songs_validation_3.pkl\n",
            "│   └── y_listened_songs_validation_4.pkl\n",
            "├── train\n",
            "│   ├── x_train_0.pkl\n",
            "│   ├── x_train_1.pkl\n",
            "│   ├── x_train_2.pkl\n",
            "│   ├── x_train_3.pkl\n",
            "│   ├── x_train_4.pkl\n",
            "│   ├── y_train_0.pkl\n",
            "│   ├── y_train_1.pkl\n",
            "│   ├── y_train_2.pkl\n",
            "│   ├── y_train_3.pkl\n",
            "│   └── y_train_4.pkl\n",
            "└── validation\n",
            "    ├── groundtruth_list_0.pkl\n",
            "    ├── groundtruth_list_1.pkl\n",
            "    ├── groundtruth_list_2.pkl\n",
            "    ├── groundtruth_list_3.pkl\n",
            "    ├── groundtruth_list_4.pkl\n",
            "    ├── x_validation_0.pkl\n",
            "    ├── x_validation_1.pkl\n",
            "    ├── x_validation_2.pkl\n",
            "    ├── x_validation_3.pkl\n",
            "    ├── x_validation_4.pkl\n",
            "    ├── y_listened_songs_validation_0.pkl\n",
            "    ├── y_listened_songs_validation_1.pkl\n",
            "    ├── y_listened_songs_validation_2.pkl\n",
            "    ├── y_listened_songs_validation_3.pkl\n",
            "    └── y_listened_songs_validation_4.pkl\n",
            "\n",
            "3 directories, 41 files\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}