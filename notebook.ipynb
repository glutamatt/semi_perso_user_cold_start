{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glutamatt/semi_perso_user_cold_start/blob/colab/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GsEYdh9xZa9",
        "outputId": "6041776b-51c3-486b-f753-f1339c96a947"
      },
      "source": [
        "!rm -fr repository deezer data\n",
        "!git clone https://github.com/glutamatt/semi_perso_user_cold_start.git repository\n",
        "!cd repository && git checkout colab && cd -"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'repository'...\n",
            "remote: Enumerating objects: 154, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/154)\u001b[K\rremote: Counting objects:   1% (2/154)\u001b[K\rremote: Counting objects:   2% (4/154)\u001b[K\rremote: Counting objects:   3% (5/154)\u001b[K\rremote: Counting objects:   4% (7/154)\u001b[K\rremote: Counting objects:   5% (8/154)\u001b[K\rremote: Counting objects:   6% (10/154)\u001b[K\rremote: Counting objects:   7% (11/154)\u001b[K\rremote: Counting objects:   8% (13/154)\u001b[K\rremote: Counting objects:   9% (14/154)\u001b[K\rremote: Counting objects:  10% (16/154)\u001b[K\rremote: Counting objects:  11% (17/154)\u001b[K\rremote: Counting objects:  12% (19/154)\u001b[K\rremote: Counting objects:  13% (21/154)\u001b[K\rremote: Counting objects:  14% (22/154)\u001b[K\rremote: Counting objects:  15% (24/154)\u001b[K\rremote: Counting objects:  16% (25/154)\u001b[K\rremote: Counting objects:  17% (27/154)\u001b[K\rremote: Counting objects:  18% (28/154)\u001b[K\rremote: Counting objects:  19% (30/154)\u001b[K\rremote: Counting objects:  20% (31/154)\u001b[K\rremote: Counting objects:  21% (33/154)\u001b[K\rremote: Counting objects:  22% (34/154)\u001b[K\rremote: Counting objects:  23% (36/154)\u001b[K\rremote: Counting objects:  24% (37/154)\u001b[K\rremote: Counting objects:  25% (39/154)\u001b[K\rremote: Counting objects:  26% (41/154)\u001b[K\rremote: Counting objects:  27% (42/154)\u001b[K\rremote: Counting objects:  28% (44/154)\u001b[K\rremote: Counting objects:  29% (45/154)\u001b[K\rremote: Counting objects:  30% (47/154)\u001b[K\rremote: Counting objects:  31% (48/154)\u001b[K\rremote: Counting objects:  32% (50/154)\u001b[K\rremote: Counting objects:  33% (51/154)\u001b[K\rremote: Counting objects:  34% (53/154)\u001b[K\rremote: Counting objects:  35% (54/154)\u001b[K\rremote: Counting objects:  36% (56/154)\u001b[K\rremote: Counting objects:  37% (57/154)\u001b[K\rremote: Counting objects:  38% (59/154)\u001b[K\rremote: Counting objects:  39% (61/154)\u001b[K\rremote: Counting objects:  40% (62/154)\u001b[K\rremote: Counting objects:  41% (64/154)\u001b[K\rremote: Counting objects:  42% (65/154)\u001b[K\rremote: Counting objects:  43% (67/154)\u001b[K\rremote: Counting objects:  44% (68/154)\u001b[K\rremote: Counting objects:  45% (70/154)\u001b[K\rremote: Counting objects:  46% (71/154)\u001b[K\rremote: Counting objects:  47% (73/154)\u001b[K\rremote: Counting objects:  48% (74/154)\u001b[K\rremote: Counting objects:  49% (76/154)\u001b[K\rremote: Counting objects:  50% (77/154)\u001b[K\rremote: Counting objects:  51% (79/154)\u001b[K\rremote: Counting objects:  52% (81/154)\u001b[K\rremote: Counting objects:  53% (82/154)\u001b[K\rremote: Counting objects:  54% (84/154)\u001b[K\rremote: Counting objects:  55% (85/154)\u001b[K\rremote: Counting objects:  56% (87/154)\u001b[K\rremote: Counting objects:  57% (88/154)\u001b[K\rremote: Counting objects:  58% (90/154)\u001b[K\rremote: Counting objects:  59% (91/154)\u001b[K\rremote: Counting objects:  60% (93/154)\u001b[K\rremote: Counting objects:  61% (94/154)\u001b[K\rremote: Counting objects:  62% (96/154)\u001b[K\rremote: Counting objects:  63% (98/154)\u001b[K\rremote: Counting objects:  64% (99/154)\u001b[K\rremote: Counting objects:  65% (101/154)\u001b[K\rremote: Counting objects:  66% (102/154)\u001b[K\rremote: Counting objects:  67% (104/154)\u001b[K\rremote: Counting objects:  68% (105/154)\u001b[K\rremote: Counting objects:  69% (107/154)\u001b[K\rremote: Counting objects:  70% (108/154)\u001b[K\rremote: Counting objects:  71% (110/154)\u001b[K\rremote: Counting objects:  72% (111/154)\u001b[K\rremote: Counting objects:  73% (113/154)\u001b[K\rremote: Counting objects:  74% (114/154)\u001b[K\rremote: Counting objects:  75% (116/154)\u001b[K\rremote: Counting objects:  76% (118/154)\u001b[K\rremote: Counting objects:  77% (119/154)\u001b[K\rremote: Counting objects:  78% (121/154)\u001b[K\rremote: Counting objects:  79% (122/154)\u001b[K\rremote: Counting objects:  80% (124/154)\u001b[K\rremote: Counting objects:  81% (125/154)\u001b[K\rremote: Counting objects:  82% (127/154)\u001b[K\rremote: Counting objects:  83% (128/154)\u001b[K\rremote: Counting objects:  84% (130/154)\u001b[K\rremote: Counting objects:  85% (131/154)\u001b[K\rremote: Counting objects:  86% (133/154)\u001b[K\rremote: Counting objects:  87% (134/154)\u001b[K\rremote: Counting objects:  88% (136/154)\u001b[K\rremote: Counting objects:  89% (138/154)\u001b[K\rremote: Counting objects:  90% (139/154)\u001b[K\rremote: Counting objects:  91% (141/154)\u001b[K\rremote: Counting objects:  92% (142/154)\u001b[K\rremote: Counting objects:  93% (144/154)\u001b[K\rremote: Counting objects:  94% (145/154)\u001b[K\rremote: Counting objects:  95% (147/154)\u001b[K\rremote: Counting objects:  96% (148/154)\u001b[K\rremote: Counting objects:  97% (150/154)\u001b[K\rremote: Counting objects:  98% (151/154)\u001b[K\rremote: Counting objects:  99% (153/154)\u001b[K\rremote: Counting objects: 100% (154/154)\u001b[K\rremote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 154 (delta 84), reused 149 (delta 80), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (154/154), 591.74 KiB | 22.76 MiB/s, done.\n",
            "Resolving deltas: 100% (84/84), done.\n",
            "Branch 'colab' set up to track remote branch 'colab' from 'origin'.\n",
            "Switched to a new branch 'colab'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvp6bBk8u_yT"
      },
      "source": [
        "# Test de contenu\n",
        "\n",
        "impec \n",
        "\n",
        "## Installation\n",
        "\n",
        "### Code\n",
        "\n",
        "```Bash\n",
        "git clone https://github.com/deezer/carousel_bandits\n",
        "cd carousel_bandits\n",
        "```\n",
        "\n",
        "Requirements: python 3, matplotlib, numpy, pandas, scipy, seaborn\n",
        "\n",
        "Media services providers, such as the music streaming platform [Deezer](https://www.deezer.com/), often leverage **swipeable carousels** to recommend personalized content to their users. These carousels are ranked lists of _L_ items or **cards** from a substantially larger catalog (of size _K_), e.g. _L_ albums, artists or playlists recommended on the homepage of the Deezer app. Only a few cards, say _L_init_ < _L_, are initially displayed to users, who can **swipe** the screen to see additional cards.\n",
        "\n",
        "Selecting the most relevant content to display in carousels is a challenging task, as the catalog is large and as users have different preferences. Also, ranking matters: some cards might not be seen by some users due to the swipeable structure.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img height=\"350\" src=\"https://raw.githubusercontent.com/deezer/carousel_bandits/master/images/carousel.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VLFN2bjpnBY",
        "outputId": "5b97d671-e5d0-4b23-d33d-63dc56cc0de7"
      },
      "source": [
        "!pip install fastparquet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fastparquet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/ce/ce91d349d724a879e2c6d5cf18e06c6cd44ce1b27be66232eee445badc4b/fastparquet-0.6.3.tar.gz (318kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 29.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.19.5)\n",
            "Collecting thrift>=0.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/1e/3284d19d7be99305eda145b8aa46b0c33244e4a496ec66440dac19f8274d/thrift-0.13.0.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.7MB/s \n",
            "\u001b[?25hCollecting cramjam>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/62/fd7cfea4e64b85709a4f0d5dadccebdcb30e0131a2c57b2a7640cd98bde7/cramjam-2.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 36.5MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.1)\n",
            "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.7/dist-packages (from thrift>=0.11.0->fastparquet) (1.15.0)\n",
            "Building wheels for collected packages: fastparquet, thrift\n",
            "  Building wheel for fastparquet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastparquet: filename=fastparquet-0.6.3-cp37-cp37m-linux_x86_64.whl size=896319 sha256=61642c70b67db2e231a5ea9e76da60a24ee01a61b48018a50a7266555bef2457\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/58/37/72cfb2147ea35b5317bbf01a1d266777a6d49c38c12f327c1b\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thrift: filename=thrift-0.13.0-cp37-cp37m-linux_x86_64.whl size=348164 sha256=1090b648c9494144f6c0072a79ef99591262b9ff9c4ac71868509a8611e7448b\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/a2/46/689ccfcf40155c23edc7cdbd9de488611c8fdf49ff34b1706e\n",
            "Successfully built fastparquet thrift\n",
            "Installing collected packages: thrift, cramjam, fsspec, fastparquet\n",
            "Successfully installed cramjam-2.3.2 fastparquet-0.6.3 fsspec-2021.6.1 thrift-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdEFHqZnseg"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn\n",
        "import time\n",
        "import statistics\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.metrics import ndcg_score, dcg_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S1sWV7oy3B"
      },
      "source": [
        "def generate(dataset_path, master_path):\n",
        "\n",
        "    #songs\n",
        "\n",
        "    song_embeddings_path = dataset_path + \"/song_embeddings.parquet\"\n",
        "    song_embeddings = pd.read_parquet(song_embeddings_path, engine = 'fastparquet').fillna(0)\n",
        "\n",
        "    if not os.path.exists(master_path+\"/m_song_dict.pkl\"):\n",
        "        song_dict = {}\n",
        "        for idx, row in song_embeddings.iterrows():\n",
        "            song_dict[row['song_index']] = idx\n",
        "        pickle.dump(song_dict, open(\"{}/m_song_dict.pkl\".format(master_path), \"wb\"))\n",
        "    else:\n",
        "        song_dict = pickle.load(open(\"{}/m_song_dict.pkl\".format(master_path), \"rb\"))\n",
        "\n",
        "\n",
        "    # user embeddings (target = only for train users)\n",
        "\n",
        "    user_embeddings = pd.read_parquet(dataset_path + \"/user_embeddings.parquet\", engine = 'fastparquet')\n",
        "    list_embeddings = [\"embedding_\"+str(i) for i in range(len(user_embeddings[\"svd_embeddings\"][0]))]\n",
        "    user_embeddings[list_embeddings] = pd.DataFrame(user_embeddings.svd_embeddings.tolist(), index= user_embeddings.index)\n",
        "    embeddings_train = user_embeddings[list_embeddings].values\n",
        "\n",
        "    # user features train\n",
        "\n",
        "    features_train_path = dataset_path + \"/user_features_train.parquet\"\n",
        "    features_train = pd.read_parquet(features_train_path, engine = 'fastparquet').fillna(0)\n",
        "    features_train = features_train.sort_values(\"user_index\")\n",
        "    features_train = features_train.reset_index(drop=True)#to check it is ok for train data\n",
        "\n",
        "    # training dataset creation\n",
        "\n",
        "    dataset = \"train\"\n",
        "    if not os.path.exists(master_path+\"/\"):\n",
        "        os.mkdir(master_path+\"/\")\n",
        "    if not os.path.exists(master_path+\"/\"+dataset+\"/\"):\n",
        "        os.mkdir(master_path+\"/\"+dataset+\"/\")\n",
        "    for idx in range(len(features_train)):\n",
        "        x_train = torch.FloatTensor(features_train.iloc[idx,2:])\n",
        "        y_train = torch.FloatTensor(user_embeddings[list_embeddings].iloc[idx,:])\n",
        "        pickle.dump(x_train, open(\"{}/{}/x_train_{}.pkl\".format(master_path, dataset, idx), \"wb\"))\n",
        "        pickle.dump(y_train, open(\"{}/{}/y_train_{}.pkl\".format(master_path, dataset, idx), \"wb\"))\n",
        "\n",
        "    # user features validation & test\n",
        "\n",
        "    for dataset in dataset_eval :\n",
        "        features_validation_path = dataset_path + \"/user_features_\" + dataset + \".parquet\"\n",
        "        features_validation = pd.read_parquet(features_validation_path, engine = 'fastparquet').fillna(0)\n",
        "        features_validation = features_validation.sort_values(\"user_index\")\n",
        "        features_validation = features_validation.reset_index(drop=True)\n",
        "\n",
        "        if not os.path.exists(master_path+\"/\"+dataset+\"/\"):\n",
        "            os.mkdir(master_path+\"/\"+dataset+\"/\"+\"/\")\n",
        "        for i in range(len(features_validation)):\n",
        "            x_validation = torch.FloatTensor(features_validation.iloc[i,2:])\n",
        "            y_validation = [song_dict[song_index]  for song_index in features_validation[\"d1d30_songs\"][i]]\n",
        "            groundtruth_validation_list = [1.0 * (song in y_validation) for song in range(len(song_embeddings))]\n",
        "            pickle.dump(x_validation, open(\"{}/{}/x_validation_{}.pkl\".format(master_path, dataset, i), \"wb\"))\n",
        "            pickle.dump(y_validation, open(\"{}/{}/y_listened_songs_validation_{}.pkl\".format(master_path, dataset, i), \"wb\"))\n",
        "            pickle.dump(groundtruth_validation_list, open(\"{}/{}/groundtruth_list_{}.pkl\".format(master_path, dataset, i), \"wb\"))\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp3NF6rMo4SR",
        "outputId": "545a8bff-abf8-46c7-e455-140a78d3a695"
      },
      "source": [
        "master_path= \"./deezer\"\n",
        "dataset_path = \"repository/data\"\n",
        "dataset_eval = [\"validation\", \"test\"]\n",
        "if not os.path.exists(\"{}/\".format(master_path)):\n",
        "  os.mkdir(\"{}/\".format(master_path))\n",
        "  # preparing dataset. It needs about XXGB of your hard disk space.\n",
        "  generate(dataset_path, master_path)\n",
        "else:\n",
        "  print(\"%s already exists\" % master_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./deezer already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFnfW4DBzmIx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f248be8c-cf39-4807-8c02-c61461d50d6b"
      },
      "source": [
        "!apt install -qqy tree && tree deezer"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 160815 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "deezer\n",
            "├── m_song_dict.pkl\n",
            "├── test\n",
            "│   ├── groundtruth_list_0.pkl\n",
            "│   ├── groundtruth_list_1.pkl\n",
            "│   ├── groundtruth_list_2.pkl\n",
            "│   ├── groundtruth_list_3.pkl\n",
            "│   ├── groundtruth_list_4.pkl\n",
            "│   ├── x_validation_0.pkl\n",
            "│   ├── x_validation_1.pkl\n",
            "│   ├── x_validation_2.pkl\n",
            "│   ├── x_validation_3.pkl\n",
            "│   ├── x_validation_4.pkl\n",
            "│   ├── y_listened_songs_validation_0.pkl\n",
            "│   ├── y_listened_songs_validation_1.pkl\n",
            "│   ├── y_listened_songs_validation_2.pkl\n",
            "│   ├── y_listened_songs_validation_3.pkl\n",
            "│   └── y_listened_songs_validation_4.pkl\n",
            "├── train\n",
            "│   ├── x_train_0.pkl\n",
            "│   ├── x_train_1.pkl\n",
            "│   ├── x_train_2.pkl\n",
            "│   ├── x_train_3.pkl\n",
            "│   ├── x_train_4.pkl\n",
            "│   ├── y_train_0.pkl\n",
            "│   ├── y_train_1.pkl\n",
            "│   ├── y_train_2.pkl\n",
            "│   ├── y_train_3.pkl\n",
            "│   └── y_train_4.pkl\n",
            "└── validation\n",
            "    ├── groundtruth_list_0.pkl\n",
            "    ├── groundtruth_list_1.pkl\n",
            "    ├── groundtruth_list_2.pkl\n",
            "    ├── groundtruth_list_3.pkl\n",
            "    ├── groundtruth_list_4.pkl\n",
            "    ├── x_validation_0.pkl\n",
            "    ├── x_validation_1.pkl\n",
            "    ├── x_validation_2.pkl\n",
            "    ├── x_validation_3.pkl\n",
            "    ├── x_validation_4.pkl\n",
            "    ├── y_listened_songs_validation_0.pkl\n",
            "    ├── y_listened_songs_validation_1.pkl\n",
            "    ├── y_listened_songs_validation_2.pkl\n",
            "    ├── y_listened_songs_validation_3.pkl\n",
            "    └── y_listened_songs_validation_4.pkl\n",
            "\n",
            "3 directories, 41 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZEJeyd74qjP"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn\n",
        "\n",
        "class RegressionTripleHidden(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, first_hidden_dim = 400, second_hidden_dim = 300, third_hidden_dim = 200, drop_out = 0):\n",
        "        super(RegressionTripleHidden, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.first_hidden_dim  = first_hidden_dim\n",
        "        self.second_hidden_dim  = second_hidden_dim\n",
        "        self.third_hidden_dim  = third_hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dpin = torch.nn.Dropout(drop_out)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(self.input_dim, self.first_hidden_dim)\n",
        "        self.fc1_bn = torch.nn.BatchNorm1d(self.first_hidden_dim)\n",
        "\n",
        "        self.fc2 = torch.nn.Linear(self.first_hidden_dim, self.second_hidden_dim)\n",
        "        self.fc2_bn = torch.nn.BatchNorm1d(self.second_hidden_dim)\n",
        "\n",
        "        self.fc3 = torch.nn.Linear(self.second_hidden_dim, self.third_hidden_dim)\n",
        "        self.fc3_bn = torch.nn.BatchNorm1d(self.third_hidden_dim)\n",
        "\n",
        "        self.fc4 = torch.nn.Linear(self.third_hidden_dim, self.output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden1 = self.fc1_bn(F.relu((self.fc1(self.dpin(x)))))\n",
        "        hidden2 = self.fc2_bn(F.relu(self.fc2(hidden1)))\n",
        "        hidden3 = self.fc3_bn(F.relu(self.fc3(hidden2)))\n",
        "        output = F.normalize(self.fc4(hidden3), dim = 1)\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjL4q-ID4f99"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "#from model import RegressionTripleHidden\n",
        "#from options import config\n",
        "\n",
        "def training(dataset_path, master_path, eval=True, model_save=True, model_filename=None):#XXXX change None\n",
        "    if config['use_cuda']:\n",
        "        cuda = torch.device(0)\n",
        "\n",
        "    input_dim = 2579 #dataset.shape[1]\n",
        "    target_dim = config['embeddings_dim']\n",
        "\n",
        "    nb_epochs = config['nb_epochs']\n",
        "    learning_rate = config['learning_rate']\n",
        "    reg_param = config['reg_param']\n",
        "    drop_out = config['drop_out']\n",
        "    batch_size = config['batch_size']\n",
        "    eval_every = config['eval_every']\n",
        "    k_val = config['k_val']\n",
        "\n",
        "    regression_model = RegressionTripleHidden(input_dim=input_dim, output_dim = target_dim).cuda(device = cuda)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(regression_model.parameters(), lr = learning_rate, weight_decay=reg_param )\n",
        "\n",
        "    if not os.path.exists(model_filename):\n",
        "\n",
        "        # Load training dataset.\n",
        "        training_set_size = int(len(os.listdir(\"{}/train\".format(master_path))) / 2)\n",
        "        train_xs = []\n",
        "        train_ys = []\n",
        "        for idx in range(training_set_size):\n",
        "            train_xs.append(pickle.load(open(\"{}/train/x_train_{}.pkl\".format(master_path, idx), \"rb\")))\n",
        "            train_ys.append(pickle.load(open(\"{}/train/y_train_{}.pkl\".format(master_path, idx), \"rb\")))\n",
        "        total_dataset = list(zip(train_xs, train_ys))\n",
        "        del(train_xs, train_ys)\n",
        "\n",
        "        if eval:\n",
        "\n",
        "            # Load validation dataset.\n",
        "\n",
        "            validation_set_size = int(len(os.listdir(\"{}/validation\".format(master_path))) / 3)\n",
        "            validation_xs = []\n",
        "            listened_songs_validation_ys = []\n",
        "            for idx in range(validation_set_size):\n",
        "                validation_xs.append(pickle.load(open(\"{}/validation/x_validation_{}.pkl\".format(master_path, idx), \"rb\")))\n",
        "                listened_songs_validation_ys.append(pickle.load(open(\"{}/validation/y_listened_songs_validation_{}.pkl\".format(master_path, idx), \"rb\")))\n",
        "            total_validation_dataset = list(zip(validation_xs, listened_songs_validation_ys))\n",
        "            del(validation_xs, listened_songs_validation_ys)\n",
        "\n",
        "            # Load song embeddings for evaluation\n",
        "\n",
        "            song_embeddings_path = dataset_path + \"/song_embeddings.parquet\"\n",
        "            song_embeddings = pd.read_parquet(song_embeddings_path, engine = 'fastparquet').fillna(0)\n",
        "            list_features = [\"feature_\"+str(i) for i in range(len(song_embeddings[\"features_svd\"][0]))]\n",
        "            song_embeddings[list_features] = pd.DataFrame(song_embeddings.features_svd.tolist(), index= song_embeddings.index)\n",
        "            song_embeddings_values = song_embeddings[list_features].values\n",
        "            song_embeddings_values_ = torch.FloatTensor(song_embeddings_values.astype(np.float32))\n",
        "\n",
        "        training_set_size = len(total_dataset)\n",
        "        print(\"training set size : \"+str(training_set_size))\n",
        "        print(\"validation set size : \"+str(validation_set_size))\n",
        "        print(\"regression model : \"+ str(regression_model))\n",
        "        print(\"training running\")\n",
        "        loss_train = []\n",
        "        for nb in range(nb_epochs):\n",
        "            print(\"nb epoch : \"+str(nb))\n",
        "            start_time_epoch = time.time()\n",
        "            random.Random(nb).shuffle(total_dataset)\n",
        "            a,b = zip(*total_dataset)\n",
        "            num_batch = int(training_set_size / batch_size)\n",
        "            max_loc = batch_size*num_batch\n",
        "            current_loss = 0\n",
        "            regression_model = regression_model.to(device = cuda)\n",
        "            for i in range(num_batch):\n",
        "                optimizer.zero_grad()\n",
        "                batch_features_tensor = torch.stack(a[batch_size*i:batch_size*(i+1)]).cuda(device = cuda)\n",
        "                batch_target_tensor = torch.stack(b[batch_size*i:batch_size*(i+1)]).cuda(device = cuda)\n",
        "                output_tensor = regression_model(batch_features_tensor)\n",
        "                loss = criterion(output_tensor, batch_target_tensor)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_train.append(loss.item())\n",
        "            print('epoch ' + str(nb) +  \" training loss : \"+ str(sum(loss_train)/float(len(loss_train))))\n",
        "            print(\"--- seconds ---\" + str(time.time() - start_time_epoch))\n",
        "\n",
        "            if nb != 0 and (nb % eval_every == 0 or nb == nb_epochs - 1):\n",
        "                print('testing model')\n",
        "                start_time_eval = time.time()\n",
        "                reg = regression_model.eval()\n",
        "                reg = reg.to(device=cuda)\n",
        "                validation_set_size = len(total_validation_dataset)\n",
        "                a,b = zip(*total_validation_dataset)\n",
        "                num_batch_validation = int(validation_set_size / batch_size)\n",
        "                current_recalls = []\n",
        "                with torch.set_grad_enabled(False):\n",
        "                    for i in range(num_batch_validation):\n",
        "                        batch_features_tensor_validation = torch.stack(a[batch_size*i:batch_size*(i+1)]).cuda(device = cuda)\n",
        "                        predictions_validation = reg(batch_features_tensor_validation)\n",
        "                        groundtruth_validation = list(b[batch_size*i:batch_size*(i+1)])\n",
        "                        predictions_songs_validation = torch.mm(predictions_validation.cpu(), song_embeddings_values_.transpose(0, 1))\n",
        "                        recommendations_validation = (predictions_songs_validation.topk(k= k_val, dim = 1)[1]).tolist()\n",
        "                        recalls = list(map(lambda x, y: len(set(x) & set(y))/float(min(len(x), 50)), groundtruth_validation, recommendations_validation))\n",
        "                        current_recalls.extend(recalls)\n",
        "                print('epoch ' + str(nb) +  \" recall test : \"+ str(sum(current_recalls) / float(len(current_recalls))) )\n",
        "                print(\"--- %s seconds ---\" + str(time.time() - start_time_eval))\n",
        "\n",
        "        if model_save:\n",
        "            torch.save(regression_model.state_dict(), master_path + \"/\"+model_filename+\".pt\")\n",
        "\n",
        "    else:\n",
        "        trained_state_dict = torch.load(master_path + \"/\"+model_filename+\".pt\")\n",
        "        RegressionTripleHidden.load_state_dict(trained_state_dict)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQDESG0C4Tpp",
        "outputId": "4dca8f2a-9a73-45fb-ae72-4e8dd991db7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "config = {\n",
        "    # user\n",
        "    'embeddings_dim': 128,\n",
        "    # cuda setting\n",
        "    'use_cuda': True,\n",
        "    # model setting\n",
        "    'nb_epochs': 130,\n",
        "    'learning_rate': 0.00001,\n",
        "    'batch_size': 2,#512,XXX to change\n",
        "    'reg_param': 0,\n",
        "    'drop_out': 0,\n",
        "    # model training\n",
        "    'eval_every': 10,\n",
        "    'k_val': 3,#50, XXX to change\n",
        "}\n",
        "\n",
        "training(dataset_path, master_path, eval=True, model_save=True, model_filename=\"20210709_svd_sample\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set size : 5\n",
            "validation set size : 5\n",
            "regression model : RegressionTripleHidden(\n",
            "  (dpin): Dropout(p=0, inplace=False)\n",
            "  (fc1): Linear(in_features=2579, out_features=400, bias=True)\n",
            "  (fc1_bn): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=400, out_features=300, bias=True)\n",
            "  (fc2_bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=300, out_features=200, bias=True)\n",
            "  (fc3_bn): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc4): Linear(in_features=200, out_features=128, bias=True)\n",
            ")\n",
            "training running\n",
            "nb epoch : 0\n",
            "epoch 0 training loss : 0.016228213906288147\n",
            "--- seconds ---0.2679743766784668\n",
            "nb epoch : 1\n",
            "epoch 1 training loss : 0.015946062747389078\n",
            "--- seconds ---0.006127595901489258\n",
            "nb epoch : 2\n",
            "epoch 2 training loss : 0.016097546244661014\n",
            "--- seconds ---0.005742549896240234\n",
            "nb epoch : 3\n",
            "epoch 3 training loss : 0.015965681755915284\n",
            "--- seconds ---0.006865262985229492\n",
            "nb epoch : 4\n",
            "epoch 4 training loss : 0.01588420057669282\n",
            "--- seconds ---0.005831003189086914\n",
            "nb epoch : 5\n",
            "epoch 5 training loss : 0.015841359505429864\n",
            "--- seconds ---0.006009340286254883\n",
            "nb epoch : 6\n",
            "epoch 6 training loss : 0.01577188161068729\n",
            "--- seconds ---0.006075143814086914\n",
            "nb epoch : 7\n",
            "epoch 7 training loss : 0.01572404458420351\n",
            "--- seconds ---0.0065882205963134766\n",
            "nb epoch : 8\n",
            "epoch 8 training loss : 0.01565392144645254\n",
            "--- seconds ---0.006732463836669922\n",
            "nb epoch : 9\n",
            "epoch 9 training loss : 0.015685666538774966\n",
            "--- seconds ---0.0064771175384521484\n",
            "nb epoch : 10\n",
            "epoch 10 training loss : 0.015695038505575874\n",
            "--- seconds ---0.006157398223876953\n",
            "testing model\n",
            "epoch 10 recall test : 0.625\n",
            "--- %s seconds ---0.01461648941040039\n",
            "nb epoch : 11\n",
            "epoch 11 training loss : 0.0157671426422894\n",
            "--- seconds ---0.0065577030181884766\n",
            "nb epoch : 12\n",
            "epoch 12 training loss : 0.015788806459078424\n",
            "--- seconds ---0.0058863162994384766\n",
            "nb epoch : 13\n",
            "epoch 13 training loss : 0.015823307713227614\n",
            "--- seconds ---0.00565791130065918\n",
            "nb epoch : 14\n",
            "epoch 14 training loss : 0.01579106304173668\n",
            "--- seconds ---0.006293535232543945\n",
            "nb epoch : 15\n",
            "epoch 15 training loss : 0.015712768159573898\n",
            "--- seconds ---0.006153106689453125\n",
            "nb epoch : 16\n",
            "epoch 16 training loss : 0.015687210731865728\n",
            "--- seconds ---0.005789995193481445\n",
            "nb epoch : 17\n",
            "epoch 17 training loss : 0.015653851855960157\n",
            "--- seconds ---0.007389545440673828\n",
            "nb epoch : 18\n",
            "epoch 18 training loss : 0.015674387815555458\n",
            "--- seconds ---0.006228446960449219\n",
            "nb epoch : 19\n",
            "epoch 19 training loss : 0.015640571038238703\n",
            "--- seconds ---0.00536799430847168\n",
            "nb epoch : 20\n",
            "epoch 20 training loss : 0.015567018960913023\n",
            "--- seconds ---0.006338357925415039\n",
            "testing model\n",
            "epoch 20 recall test : 0.625\n",
            "--- %s seconds ---0.0020494461059570312\n",
            "nb epoch : 21\n",
            "epoch 21 training loss : 0.015505611663684249\n",
            "--- seconds ---0.0065119266510009766\n",
            "nb epoch : 22\n",
            "epoch 22 training loss : 0.015428402542096117\n",
            "--- seconds ---0.007100820541381836\n",
            "nb epoch : 23\n",
            "epoch 23 training loss : 0.01537649294671913\n",
            "--- seconds ---0.005438327789306641\n",
            "nb epoch : 24\n",
            "epoch 24 training loss : 0.015365210194140673\n",
            "--- seconds ---0.006005764007568359\n",
            "nb epoch : 25\n",
            "epoch 25 training loss : 0.015265864552929997\n",
            "--- seconds ---0.0062999725341796875\n",
            "nb epoch : 26\n",
            "epoch 26 training loss : 0.015192572355132413\n",
            "--- seconds ---0.0062482357025146484\n",
            "nb epoch : 27\n",
            "epoch 27 training loss : 0.015095935336181096\n",
            "--- seconds ---0.006367683410644531\n",
            "nb epoch : 28\n",
            "epoch 28 training loss : 0.01502210855612467\n",
            "--- seconds ---0.009795427322387695\n",
            "nb epoch : 29\n",
            "epoch 29 training loss : 0.014947746957962712\n",
            "--- seconds ---0.006537914276123047\n",
            "nb epoch : 30\n",
            "epoch 30 training loss : 0.014872791290643715\n",
            "--- seconds ---0.005746126174926758\n",
            "testing model\n",
            "epoch 30 recall test : 0.75\n",
            "--- %s seconds ---0.0024263858795166016\n",
            "nb epoch : 31\n",
            "epoch 31 training loss : 0.014788547254283912\n",
            "--- seconds ---0.0068247318267822266\n",
            "nb epoch : 32\n",
            "epoch 32 training loss : 0.014724667621494242\n",
            "--- seconds ---0.005892038345336914\n",
            "nb epoch : 33\n",
            "epoch 33 training loss : 0.014659664730596192\n",
            "--- seconds ---0.006103038787841797\n",
            "nb epoch : 34\n",
            "epoch 34 training loss : 0.014593824917184455\n",
            "--- seconds ---0.006204366683959961\n",
            "nb epoch : 35\n",
            "epoch 35 training loss : 0.014527174695912335\n",
            "--- seconds ---0.0056915283203125\n",
            "nb epoch : 36\n",
            "epoch 36 training loss : 0.014450270380522753\n",
            "--- seconds ---0.008569955825805664\n",
            "nb epoch : 37\n",
            "epoch 37 training loss : 0.014373586378305367\n",
            "--- seconds ---0.005840778350830078\n",
            "nb epoch : 38\n",
            "epoch 38 training loss : 0.014305451096823582\n",
            "--- seconds ---0.006537437438964844\n",
            "nb epoch : 39\n",
            "epoch 39 training loss : 0.014270367531571538\n",
            "--- seconds ---0.006153106689453125\n",
            "nb epoch : 40\n",
            "epoch 40 training loss : 0.014187909549147618\n",
            "--- seconds ---0.0068111419677734375\n",
            "testing model\n",
            "epoch 40 recall test : 0.75\n",
            "--- %s seconds ---0.002173900604248047\n",
            "nb epoch : 41\n",
            "epoch 41 training loss : 0.014111317483530868\n",
            "--- seconds ---0.0062656402587890625\n",
            "nb epoch : 42\n",
            "epoch 42 training loss : 0.01403010773017656\n",
            "--- seconds ---0.00580143928527832\n",
            "nb epoch : 43\n",
            "epoch 43 training loss : 0.013991695680570874\n",
            "--- seconds ---0.006324052810668945\n",
            "nb epoch : 44\n",
            "epoch 44 training loss : 0.013922639718900124\n",
            "--- seconds ---0.005884885787963867\n",
            "nb epoch : 45\n",
            "epoch 45 training loss : 0.013881743672992225\n",
            "--- seconds ---0.005829572677612305\n",
            "nb epoch : 46\n",
            "epoch 46 training loss : 0.01380583030627446\n",
            "--- seconds ---0.006173610687255859\n",
            "nb epoch : 47\n",
            "epoch 47 training loss : 0.013763278780970722\n",
            "--- seconds ---0.005269765853881836\n",
            "nb epoch : 48\n",
            "epoch 48 training loss : 0.013719382230192423\n",
            "--- seconds ---0.005297183990478516\n",
            "nb epoch : 49\n",
            "epoch 49 training loss : 0.013649983080103994\n",
            "--- seconds ---0.0059032440185546875\n",
            "nb epoch : 50\n",
            "epoch 50 training loss : 0.013574976318826279\n",
            "--- seconds ---0.005934000015258789\n",
            "testing model\n",
            "epoch 50 recall test : 0.75\n",
            "--- %s seconds ---0.0024118423461914062\n",
            "nb epoch : 51\n",
            "epoch 51 training loss : 0.013496197966070702\n",
            "--- seconds ---0.005806446075439453\n",
            "nb epoch : 52\n",
            "epoch 52 training loss : 0.013428410093458194\n",
            "--- seconds ---0.0067234039306640625\n",
            "nb epoch : 53\n",
            "epoch 53 training loss : 0.013361010924671535\n",
            "--- seconds ---0.005666971206665039\n",
            "nb epoch : 54\n",
            "epoch 54 training loss : 0.013284073634581133\n",
            "--- seconds ---0.006860256195068359\n",
            "nb epoch : 55\n",
            "epoch 55 training loss : 0.013213327877955245\n",
            "--- seconds ---0.008658170700073242\n",
            "nb epoch : 56\n",
            "epoch 56 training loss : 0.013143223195679878\n",
            "--- seconds ---0.005767107009887695\n",
            "nb epoch : 57\n",
            "epoch 57 training loss : 0.013077590662729123\n",
            "--- seconds ---0.006260395050048828\n",
            "nb epoch : 58\n",
            "epoch 58 training loss : 0.013012367517734736\n",
            "--- seconds ---0.010079622268676758\n",
            "nb epoch : 59\n",
            "epoch 59 training loss : 0.01294389112930124\n",
            "--- seconds ---0.007082223892211914\n",
            "nb epoch : 60\n",
            "epoch 60 training loss : 0.012870608886215286\n",
            "--- seconds ---0.005300998687744141\n",
            "testing model\n",
            "epoch 60 recall test : 0.75\n",
            "--- %s seconds ---0.003693819046020508\n",
            "nb epoch : 61\n",
            "epoch 61 training loss : 0.012803423195897091\n",
            "--- seconds ---0.00707697868347168\n",
            "nb epoch : 62\n",
            "epoch 62 training loss : 0.012736847912449212\n",
            "--- seconds ---0.005977153778076172\n",
            "nb epoch : 63\n",
            "epoch 63 training loss : 0.012674237485043705\n",
            "--- seconds ---0.0054743289947509766\n",
            "nb epoch : 64\n",
            "epoch 64 training loss : 0.012608644921475879\n",
            "--- seconds ---0.0059795379638671875\n",
            "nb epoch : 65\n",
            "epoch 65 training loss : 0.012544668254894063\n",
            "--- seconds ---0.006103038787841797\n",
            "nb epoch : 66\n",
            "epoch 66 training loss : 0.012480172839488334\n",
            "--- seconds ---0.00568079948425293\n",
            "nb epoch : 67\n",
            "epoch 67 training loss : 0.012435222520520362\n",
            "--- seconds ---0.006078004837036133\n",
            "nb epoch : 68\n",
            "epoch 68 training loss : 0.012371571376865757\n",
            "--- seconds ---0.005692243576049805\n",
            "nb epoch : 69\n",
            "epoch 69 training loss : 0.012312139710411429\n",
            "--- seconds ---0.0056955814361572266\n",
            "nb epoch : 70\n",
            "epoch 70 training loss : 0.012267557044946392\n",
            "--- seconds ---0.005800485610961914\n",
            "testing model\n",
            "epoch 70 recall test : 0.75\n",
            "--- %s seconds ---0.0019125938415527344\n",
            "nb epoch : 71\n",
            "epoch 71 training loss : 0.012205338241377225\n",
            "--- seconds ---0.005724906921386719\n",
            "nb epoch : 72\n",
            "epoch 72 training loss : 0.01216093618550325\n",
            "--- seconds ---0.005843400955200195\n",
            "nb epoch : 73\n",
            "epoch 73 training loss : 0.012103413124694614\n",
            "--- seconds ---0.00638580322265625\n",
            "nb epoch : 74\n",
            "epoch 74 training loss : 0.012046292846401532\n",
            "--- seconds ---0.0068187713623046875\n",
            "nb epoch : 75\n",
            "epoch 75 training loss : 0.011982288548575812\n",
            "--- seconds ---0.005825042724609375\n",
            "nb epoch : 76\n",
            "epoch 76 training loss : 0.0119229832772988\n",
            "--- seconds ---0.006158351898193359\n",
            "nb epoch : 77\n",
            "epoch 77 training loss : 0.011879470957538638\n",
            "--- seconds ---0.006344318389892578\n",
            "nb epoch : 78\n",
            "epoch 78 training loss : 0.011824296085942017\n",
            "--- seconds ---0.005772113800048828\n",
            "nb epoch : 79\n",
            "epoch 79 training loss : 0.011762455923599192\n",
            "--- seconds ---0.005812168121337891\n",
            "nb epoch : 80\n",
            "epoch 80 training loss : 0.0117082020158615\n",
            "--- seconds ---0.005808591842651367\n",
            "testing model\n",
            "epoch 80 recall test : 0.625\n",
            "--- %s seconds ---0.0019626617431640625\n",
            "nb epoch : 81\n",
            "epoch 81 training loss : 0.011654308522915149\n",
            "--- seconds ---0.0057222843170166016\n",
            "nb epoch : 82\n",
            "epoch 82 training loss : 0.011593927216929305\n",
            "--- seconds ---0.005902528762817383\n",
            "nb epoch : 83\n",
            "epoch 83 training loss : 0.011539855154253365\n",
            "--- seconds ---0.006562709808349609\n",
            "nb epoch : 84\n",
            "epoch 84 training loss : 0.011497614302617662\n",
            "--- seconds ---0.005806922912597656\n",
            "nb epoch : 85\n",
            "epoch 85 training loss : 0.011438527436883644\n",
            "--- seconds ---0.008526086807250977\n",
            "nb epoch : 86\n",
            "epoch 86 training loss : 0.011384442068593598\n",
            "--- seconds ---0.005994558334350586\n",
            "nb epoch : 87\n",
            "epoch 87 training loss : 0.011326084712477908\n",
            "--- seconds ---0.009369611740112305\n",
            "nb epoch : 88\n",
            "epoch 88 training loss : 0.011272954843477921\n",
            "--- seconds ---0.006789684295654297\n",
            "nb epoch : 89\n",
            "epoch 89 training loss : 0.01122153773645146\n",
            "--- seconds ---0.007220029830932617\n",
            "nb epoch : 90\n",
            "epoch 90 training loss : 0.011180285929835268\n",
            "--- seconds ---0.0070378780364990234\n",
            "testing model\n",
            "epoch 90 recall test : 0.625\n",
            "--- %s seconds ---0.0028994083404541016\n",
            "nb epoch : 91\n",
            "epoch 91 training loss : 0.011123452001534726\n",
            "--- seconds ---0.00645899772644043\n",
            "nb epoch : 92\n",
            "epoch 92 training loss : 0.011071967083700401\n",
            "--- seconds ---0.007006168365478516\n",
            "nb epoch : 93\n",
            "epoch 93 training loss : 0.011022159762858868\n",
            "--- seconds ---0.007198810577392578\n",
            "nb epoch : 94\n",
            "epoch 94 training loss : 0.010972595182982714\n",
            "--- seconds ---0.0076792240142822266\n",
            "nb epoch : 95\n",
            "epoch 95 training loss : 0.010923415740156392\n",
            "--- seconds ---0.006318092346191406\n",
            "nb epoch : 96\n",
            "epoch 96 training loss : 0.010873542752923425\n",
            "--- seconds ---0.006758213043212891\n",
            "nb epoch : 97\n",
            "epoch 97 training loss : 0.010833558818914605\n",
            "--- seconds ---0.0067920684814453125\n",
            "nb epoch : 98\n",
            "epoch 98 training loss : 0.010784285789769556\n",
            "--- seconds ---0.006727695465087891\n",
            "nb epoch : 99\n",
            "epoch 99 training loss : 0.010730613723862916\n",
            "--- seconds ---0.006051778793334961\n",
            "nb epoch : 100\n",
            "epoch 100 training loss : 0.010685009725096792\n",
            "--- seconds ---0.0069425106048583984\n",
            "testing model\n",
            "epoch 100 recall test : 0.625\n",
            "--- %s seconds ---0.0026378631591796875\n",
            "nb epoch : 101\n",
            "epoch 101 training loss : 0.010639802404387179\n",
            "--- seconds ---0.006822824478149414\n",
            "nb epoch : 102\n",
            "epoch 102 training loss : 0.010592037280659798\n",
            "--- seconds ---0.006814718246459961\n",
            "nb epoch : 103\n",
            "epoch 103 training loss : 0.010539933602558449\n",
            "--- seconds ---0.006396055221557617\n",
            "nb epoch : 104\n",
            "epoch 104 training loss : 0.010492946890493234\n",
            "--- seconds ---0.00664973258972168\n",
            "nb epoch : 105\n",
            "epoch 105 training loss : 0.010446320856780798\n",
            "--- seconds ---0.005702018737792969\n",
            "nb epoch : 106\n",
            "epoch 106 training loss : 0.010401146392834104\n",
            "--- seconds ---0.0057735443115234375\n",
            "nb epoch : 107\n",
            "epoch 107 training loss : 0.010363212704693002\n",
            "--- seconds ---0.007521629333496094\n",
            "nb epoch : 108\n",
            "epoch 108 training loss : 0.010320522719908744\n",
            "--- seconds ---0.006842851638793945\n",
            "nb epoch : 109\n",
            "epoch 109 training loss : 0.01027632361828265\n",
            "--- seconds ---0.006638288497924805\n",
            "nb epoch : 110\n",
            "epoch 110 training loss : 0.010238856265975817\n",
            "--- seconds ---0.006063222885131836\n",
            "testing model\n",
            "epoch 110 recall test : 0.625\n",
            "--- %s seconds ---0.0017733573913574219\n",
            "nb epoch : 111\n",
            "epoch 111 training loss : 0.010194213297966468\n",
            "--- seconds ---0.006412506103515625\n",
            "nb epoch : 112\n",
            "epoch 112 training loss : 0.010150860185298113\n",
            "--- seconds ---0.006140470504760742\n",
            "nb epoch : 113\n",
            "epoch 113 training loss : 0.010109788795377602\n",
            "--- seconds ---0.006708383560180664\n",
            "nb epoch : 114\n",
            "epoch 114 training loss : 0.010061735897729901\n",
            "--- seconds ---0.006726741790771484\n",
            "nb epoch : 115\n",
            "epoch 115 training loss : 0.010021273963000819\n",
            "--- seconds ---0.013538837432861328\n",
            "nb epoch : 116\n",
            "epoch 116 training loss : 0.009981124284756808\n",
            "--- seconds ---0.006782054901123047\n",
            "nb epoch : 117\n",
            "epoch 117 training loss : 0.00993848917907167\n",
            "--- seconds ---0.0058629512786865234\n",
            "nb epoch : 118\n",
            "epoch 118 training loss : 0.009902483997905642\n",
            "--- seconds ---0.005923032760620117\n",
            "nb epoch : 119\n",
            "epoch 119 training loss : 0.009861224824756694\n",
            "--- seconds ---0.005645275115966797\n",
            "nb epoch : 120\n",
            "epoch 120 training loss : 0.009820228214529613\n",
            "--- seconds ---0.005909919738769531\n",
            "testing model\n",
            "epoch 120 recall test : 0.625\n",
            "--- %s seconds ---0.0026578903198242188\n",
            "nb epoch : 121\n",
            "epoch 121 training loss : 0.00978467151900509\n",
            "--- seconds ---0.0061376094818115234\n",
            "nb epoch : 122\n",
            "epoch 122 training loss : 0.009749242309973068\n",
            "--- seconds ---0.006647348403930664\n",
            "nb epoch : 123\n",
            "epoch 123 training loss : 0.009708978634725715\n",
            "--- seconds ---0.0062906742095947266\n",
            "nb epoch : 124\n",
            "epoch 124 training loss : 0.009673784331418574\n",
            "--- seconds ---0.009846687316894531\n",
            "nb epoch : 125\n",
            "epoch 125 training loss : 0.009633965988916951\n",
            "--- seconds ---0.0063076019287109375\n",
            "nb epoch : 126\n",
            "epoch 126 training loss : 0.00959433716864831\n",
            "--- seconds ---0.00614619255065918\n",
            "nb epoch : 127\n",
            "epoch 127 training loss : 0.009559587199873931\n",
            "--- seconds ---0.006127595901489258\n",
            "nb epoch : 128\n",
            "epoch 128 training loss : 0.009522412736654686\n",
            "--- seconds ---0.006341695785522461\n",
            "nb epoch : 129\n",
            "epoch 129 training loss : 0.0094854742834846\n",
            "--- seconds ---0.005839824676513672\n",
            "testing model\n",
            "epoch 129 recall test : 0.625\n",
            "--- %s seconds ---0.0018491744995117188\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}