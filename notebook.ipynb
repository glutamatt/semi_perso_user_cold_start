{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glutamatt/semi_perso_user_cold_start/blob/colab/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvp6bBk8u_yT"
      },
      "source": [
        "# Test de contenu\n",
        "\n",
        "impec \n",
        "\n",
        "## Installation\n",
        "\n",
        "### Code\n",
        "\n",
        "```Bash\n",
        "git clone https://github.com/deezer/carousel_bandits\n",
        "cd carousel_bandits\n",
        "```\n",
        "\n",
        "Requirements: python 3, matplotlib, numpy, pandas, scipy, seaborn\n",
        "\n",
        "Media services providers, such as the music streaming platform [Deezer](https://www.deezer.com/), often leverage **swipeable carousels** to recommend personalized content to their users. These carousels are ranked lists of _L_ items or **cards** from a substantially larger catalog (of size _K_), e.g. _L_ albums, artists or playlists recommended on the homepage of the Deezer app. Only a few cards, say _L_init_ < _L_, are initially displayed to users, who can **swipe** the screen to see additional cards.\n",
        "\n",
        "Selecting the most relevant content to display in carousels is a challenging task, as the catalog is large and as users have different preferences. Also, ranking matters: some cards might not be seen by some users due to the swipeable structure.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img height=\"350\" src=\"https://raw.githubusercontent.com/deezer/carousel_bandits/master/images/carousel.png\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VLFN2bjpnBY",
        "outputId": "1d7bab74-4b49-4ce6-9ffa-602e5001917d"
      },
      "source": [
        "!pip install fastparquet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.7/dist-packages (0.6.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2021.6.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (1.19.5)\n",
            "Requirement already satisfied: thrift>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (0.13.0)\n",
            "Requirement already satisfied: cramjam>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from fastparquet) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->fastparquet) (2018.9)\n",
            "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.7/dist-packages (from thrift>=0.11.0->fastparquet) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdEFHqZnseg"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn\n",
        "import time\n",
        "import statistics\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.metrics import ndcg_score, dcg_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import Normalizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZZN20sCp8q8",
        "outputId": "7a7d6287-a8c9-4a74-e3c2-94674d82e054"
      },
      "source": [
        "!mkdir data\n",
        "!wget -O data/song_embeddings.parquet https://github.com/glutamatt/semi_perso_user_cold_start/raw/master/data/user_features_validation.parquet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "--2021-07-08 15:07:44--  https://github.com/glutamatt/semi_perso_user_cold_start/raw/master/data/user_features_validation.parquet\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/glutamatt/semi_perso_user_cold_start/master/data/user_features_validation.parquet [following]\n",
            "--2021-07-08 15:07:44--  https://raw.githubusercontent.com/glutamatt/semi_perso_user_cold_start/master/data/user_features_validation.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 770115 (752K) [application/octet-stream]\n",
            "Saving to: ‘data/song_embeddings.parquet’\n",
            "\n",
            "data/song_embedding 100%[===================>] 752.07K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-07-08 15:07:44 (8.48 MB/s) - ‘data/song_embeddings.parquet’ saved [770115/770115]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6S1sWV7oy3B"
      },
      "source": [
        "def generate(master_path):\n",
        "    dataset_path = os.getcwd() + \"/data\"\n",
        "\n",
        "    #songs\n",
        "\n",
        "    song_embeddings_path = dataset_path + \"/song_embeddings.parquet\"\n",
        "    song_embeddings = pd.read_parquet(song_embeddings_path, engine = 'fastparquet').fillna(0)\n",
        "\n",
        "    if not os.path.exists(master_path+\"/m_song_dict.pkl\"):\n",
        "        song_dict = {}\n",
        "        for idx, row in song_embeddings.iterrows():\n",
        "            print(row['song_index'])\n",
        "            song_dict[row['song_index']] = idx\n",
        "        pickle.dump(song_dict, open(\"{}/m_song_dict.pkl\".format(master_path), \"wb\"))\n",
        "    else:\n",
        "        song_dict = pickle.load(open(\"{}/m_song_dict.pkl\".format(master_path), \"rb\"))\n",
        "\n",
        "\n",
        "    # user embeddings (target = only for train users)\n",
        "\n",
        "    user_embeddings = pd.read_parquet(dataset_path + \"/user_embeddings.parquet\", engine = 'fastparquet')\n",
        "    list_embeddings = [\"embedding_\"+str(i) for i in range(len(user_embeddings[\"svd_embeddings\"][0]))]\n",
        "    user_embeddings[list_embeddings] = pd.DataFrame(user_embeddings.svd_embeddings.tolist(), index= user_embeddings.index)\n",
        "    embeddings_train = user_embeddings[list_embeddings].values\n",
        "\n",
        "    # user features train\n",
        "\n",
        "    features_train_path = dataset_path + \"/user_features_train.parquet\"\n",
        "    features_train = pd.read_parquet(features_train_path, engine = 'fastparquet').fillna(0)\n",
        "    features_train = features_train.sort_values(\"user_index\")\n",
        "    features_train = features_train.reset_index(drop=True)#to check it is ok for train data\n",
        "\n",
        "    # training dataset creation\n",
        "\n",
        "    state = \"train\"\n",
        "    if not os.path.exists(master_path+\"/\"):\n",
        "        os.mkdir(master_path+\"/\")\n",
        "    if not os.path.exists(master_path+\"/\"+state+\"/\"):\n",
        "        os.mkdir(master_path+\"/\"+state+\"/\")\n",
        "    for idx in range(len(features_train)):\n",
        "        x_train = torch.FloatTensor(features_train.iloc[idx,2:])\n",
        "        y_train = torch.FloatTensor(user_embeddings[list_embeddings].iloc[idx,:])\n",
        "        pickle.dump(x_train, open(\"{}/{}/x_train_{}.pkl\".format(master_path, state, idx), \"wb\"))\n",
        "        pickle.dump(y_train, open(\"{}/{}/y_train_{}.pkl\".format(master_path, state, idx), \"wb\"))\n",
        "\n",
        "    # user features validation & test\n",
        "\n",
        "    states = [\"validation\", \"test\"]\n",
        "    for state in states :\n",
        "        features_validation_path = dataset_path + \"/user_features_\" + state + \".parquet\"\n",
        "        features_validation = pd.read_parquet(features_validation_path, engine = 'fastparquet').fillna(0)\n",
        "        features_validation = features_validation.sort_values(\"user_index\")\n",
        "        features_validation = features_validation.reset_index(drop=True)\n",
        "\n",
        "        if not os.path.exists(master_path+\"/\"+state+\"/\"):\n",
        "            os.mkdir(master_path+\"/\"+state+\"/\"+\"/\")\n",
        "        for i in range(len(features_validation)):\n",
        "            x_validation = torch.FloatTensor(features_validation.iloc[i,2:])\n",
        "            y_validation = [song_dict[song_index]  for song_index in features_validation[\"d1d30_songs\"][i]]\n",
        "            groundtruth_validation_list = [1.0 * (song in y_validation) for song in range(len(song_embeddings))]\n",
        "            pickle.dump(x_validation, open(\"{}/{}/x_validation_{}.pkl\".format(master_path, state, i), \"wb\"))\n",
        "            pickle.dump(y_validation, open(\"{}/{}/y_listened_songs_validation_{}.pkl\".format(master_path, state, i), \"wb\"))\n",
        "            pickle.dump(groundtruth_validation_list, open(\"{}/{}/groundtruth_list_{}.pkl\".format(master_path, state, i), \"wb\"))\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "cp3NF6rMo4SR",
        "outputId": "6e1da945-76ff-42b8-c59e-af60f26bf169"
      },
      "source": [
        "master_path= \"./deezer\"\n",
        "if not os.path.exists(\"{}/\".format(master_path)):\n",
        "  os.mkdir(\"{}/\".format(master_path))\n",
        "  # preparing dataset. It needs about XXGB of your hard disk space.\n",
        "  generate(master_path)\n",
        "else:\n",
        "  print(\"%s already exists\" % master_path)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'song_index'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0a494e05cbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# preparing dataset. It needs about XXGB of your hard disk space.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmaster_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-183d65d25a38>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(master_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msong_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msong_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'song_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0msong_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'song_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msong_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/m_song_dict.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'song_index'"
          ]
        }
      ]
    }
  ]
}